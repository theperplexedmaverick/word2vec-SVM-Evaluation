{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Term embeddings + SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "\n",
    "For this homework, we will still play with Yelp reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge). As in Homework 1, you'll see that each line corresponds to a review on a particular business. Each review has a unique \"ID\" and the text content is in the \"review\" field. Additionally, this time, we also offer you the \"label\". If `label=1`, it means that this review is `Food-relevant`. If `label=0`, it means that this review is `Food-irrelevant`. Similarly, we have already done some basic preprocessing on the reviews, so you can just tokenize each review using whitespace.\n",
    "\n",
    "There are about 40,000 reviews in total, in which about 20,000 reviews are \"Food-irrelevant\". We split the review data into two sets. *review_train.json* is the training set. *review_test.json* is the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please load the dataset\n",
    "# Your code below\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "data_train = pd.read_json('review_train.json', lines=True)#Importing dataset for training\n",
    "[ids_train,reviews_train,labels_train]=data_train['id'],data_train['review'],data_train['label']\n",
    "data_test = pd.read_json('review_test.json', lines=True)#Importing dataset for testing\n",
    "[ids_test,reviews_test,labels_test]=data_test['id'],data_test['review'],data_test['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Pre-trained term embeddings\n",
    "\n",
    "To save your time, you can make use of  pre-trained term embeddings. In this homework, we are using one of the great pre-trained models from [GloVe](https://nlp.stanford.edu/projects/glove/) based on 2 billion tweets. GloVe is quite similar to word2vec. Unzip the *glove.6B.50d.txt.zip* file and run the code below. You will be able to load the term embeddings model, with which each word can be represented with a 50-dimension vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the pre-trained term embeddings\n",
    "import numpy as np\n",
    "\n",
    "with open(\"glove.6B.50d.txt\", \"rb\") as lines:\n",
    "    model = {str(line.split()[0],'utf-8'): np.array(list(map(float, line.split()[1:])))\n",
    "           for line in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you have a vector representation for each word. First, we use the simple (arithmetic) **mean** of these vectors of words in a review to represent the review. *Note: Just ignore those words which are not in the corpus of this pre-trained model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please figure out the vector representation for each review in the training data and testing data.\n",
    "# Your code below\n",
    "def vector(reviews):\n",
    "    vecrep=[[]]*len(reviews)#Initialize a 2Darray for the vector rep,every row is a vec-rep of a review\n",
    "    for idx,review in enumerate(reviews):\n",
    "        b=[]#Matrix to hold vecreps of the words in the review\n",
    "        for word in review.split():\n",
    "            if word in model.keys():#Check if the word has a vecrep in the GloVe model\n",
    "                 b.append(model[word])#Add vec-rep for the word in b\n",
    "        vecrep[idx]=[elem/len(b) for elem in np.sum(b,axis=0)]#Taking the mean of all the words,col by col\n",
    "    return vecrep\n",
    "vecrep_train=vector(reviews_train)\n",
    "vecrep_test=vector(reviews_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "With the vector representations you get for each review, please train an SVM model to predict whether a given review is food-relevant or not. **You do not need to implement any classifier from scratch. You may use scikit-learn's built-in capabilities.** You can only train your model with reviews in *review_train.json*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=5, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model training\n",
    "# Your code here\n",
    "from array import *\n",
    "from sklearn import svm\n",
    "Y=[labels_train[elem] for elem in range(len(labels_train))]\n",
    "clf = svm.SVC(C=5,kernel='linear')#Increasing the penalty+using a linear kernel for the SVM\n",
    "clf.fit(vecrep_train, Y)#Training the SVM model with the training dataset vecrep and the training labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal is to predict whether a given review is food-relevant or not. Please report the overall accuracy, precision and recall of your model on the **testing data**. You should **implement the functions for accuracy, precision, and recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907634\n",
      "Recall: 0.894977 Precision: 0.923129\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from collections import Counter\n",
    "pred=clf.predict(vecrep_test)\n",
    "def accuracy(predicted_labels, actual_labels):\n",
    "    diff = np.array(predicted_labels) - np.array(actual_labels)\n",
    "    return 1.0 - (float(np.count_nonzero(diff)) / len(diff))\n",
    "def evaluation(predicted_labels, actual_labels):\n",
    "    counts = Counter(zip(actual_labels, predicted_labels))\n",
    "    true_pos=counts[1,1]\n",
    "    false_neg=counts[0,1]\n",
    "    false_pos=counts[1,0]\n",
    "    true_neg=counts[0,0]\n",
    "    recall = true_pos / float(true_pos + false_neg)\n",
    "    precision = true_pos / float(true_pos + false_pos)\n",
    "    print(\"Recall: %f\"%recall,\"Precision: %f\"%precision)\n",
    "Y_test=[labels_test[elem] for elem in range(len(labels_test))]\n",
    "print(\"Accuracy: %f\"%accuracy(pred,Y_test))\n",
    "evaluation(pred,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-based embeddings\n",
    "\n",
    "Instead of taking the mean of term embeddings, you can directly train a **doc2vec** model for paragraph or document embeddings. You can refer to the paper [Distributed Representations of Sentences and Documents](https://arxiv.org/pdf/1405.4053v2.pdf) for more details. And in this homework, you can make use of the implementation in [gensim](https://radimrehurek.com/gensim/models/doc2vec.html).\n",
    "\n",
    "Now, you need to:\n",
    "* Train a doc2vec model based on all reviews you have (training + testing sets).\n",
    "* Use the embeddings from your doc2vec model to represent each review and train a new SVM model.\n",
    "* Report the overall accuracy, precision and recall of your model on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\atrey\\anaconda3\\lib\\site-packages (3.7.1)\n",
      "Requirement not upgraded as not directly required: scipy>=0.18.1 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from gensim) (1.1.0)\n",
      "Requirement not upgraded as not directly required: smart-open>=1.7.0 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from gensim) (1.8.0)\n",
      "Requirement not upgraded as not directly required: six>=1.5.0 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from gensim) (1.11.0)\n",
      "Requirement not upgraded as not directly required: numpy>=1.11.3 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from gensim) (1.14.3)\n",
      "Requirement not upgraded as not directly required: bz2file in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (0.98)\n",
      "Requirement not upgraded as not directly required: boto>=2.32 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.48.0)\n",
      "Requirement not upgraded as not directly required: boto3 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (1.9.101)\n",
      "Requirement not upgraded as not directly required: requests in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.18.4)\n",
      "Requirement not upgraded as not directly required: s3transfer<0.3.0,>=0.2.0 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.0)\n",
      "Requirement not upgraded as not directly required: botocore<1.13.0,>=1.12.101 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.101)\n",
      "Requirement not upgraded as not directly required: jmespath<1.0.0,>=0.7.1 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.6)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.22)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2018.4.16)\n",
      "Requirement not upgraded as not directly required: docutils>=0.10 in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.101->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Requirement not upgraded as not directly required: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\atrey\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.101->boto3->smart-open>=1.7.0->gensim) (2.7.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed 1.21.8 requires msgpack, which is not installed.\n",
      "You are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atrey\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atrey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained\n"
     ]
    }
   ],
   "source": [
    "# Train a doc2vec\n",
    "# Your code here\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "tagged_data = [TaggedDocument(words=_d.split(), tags=[ids_train[i]])\n",
    "               for i, _d in enumerate(reviews_train)]#Splitting the review into a BOW and Using the IDs as tags\n",
    "tagged_test = [TaggedDocument(words=_d.split(), tags=[ids_test[i]]) \n",
    "               for i, _d in enumerate(reviews_test)]\n",
    "tagged_data=tagged_data+tagged_test#Total tagged data=tagged training+testing data\n",
    "model_doc2vec = Doc2Vec(tagged_data,size=20,window=5,\n",
    "                alpha=0.025, \n",
    "                min_alpha=0.0025,\n",
    "                min_count=1,\n",
    "                dm =1,epochs=50)#Setting epoch size as 50\n",
    "\n",
    "print ('Start training process...')\n",
    "model_doc2vec.train(tagged_data, total_examples=model_doc2vec.corpus_count, epochs=model_doc2vec.iter)\n",
    "print ('Model Trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Trained\n"
     ]
    }
   ],
   "source": [
    "# Train a SVM\n",
    "# Your code here\n",
    "Y_doc2vec=[labels_train[elem] for elem in range(len(labels_train))]\n",
    "#Building the training dataset by getting the vec-reps through the review ids\n",
    "vecrep_train_doc2vec=[model_doc2vec.docvecs[ids_train[elem]] for elem in range(len(reviews_train))]\n",
    "#Building a new SVM model for doc2vec\n",
    "clf_doc2vec = svm.SVC(C=5,kernel='linear')\n",
    "clf_doc2vec.fit(vecrep_train_doc2vec, Y_doc2vec)  \n",
    "print(\"SVM Model Trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.937836\n",
      "Recall: 0.937017 Precision: 0.938436\n"
     ]
    }
   ],
   "source": [
    "# Report the performance\n",
    "# Your code here\n",
    "#Building the test dataset by getting the vec-reps through the review ids\n",
    "vecrep_test_doc2vec=[model_doc2vec.docvecs[ids_test[elem]] for elem in range(len(reviews_test))]\n",
    "#Predicting the result using the new SVM model created\n",
    "pred_doc2vec=clf_doc2vec.predict(vecrep_test_doc2vec)\n",
    "#Evaluating performance\n",
    "Y_test=[labels_test[elem] for elem in range(len(labels_test))]\n",
    "print(\"Accuracy: %f\"%accuracy(pred_doc2vec,Y_test))\n",
    "evaluation(pred_doc2vec,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? How different are your results for the term-based average approach vs. the doc2vec approach? Why do you think this is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:-\n",
    "A marked difference in the accuracy can be observed between the 2 given approaches. the accuracy increases from 90.7% to 93.7%. Same goes for the recall and precision.\n",
    "The doc2vec approach performs better because this approach basically employs a neural network that learns and thus is forced to create embeddings which better reflect the relationship between words. Hence, doc2vec is a predictive model and learns to embed words accurately. However, the GloVe model mainly depends on the term frequency of words. It learns by constructing a co-occurrence matrix (words * context) that basically counts how frequently a word appears in a context and then doing dimensionality reduction on these matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you do better?\n",
    "\n",
    "Finally, see if you can do better than either the word- or doc- based embeddings approach for classification. You may explore new features, new classifiers, etc. Whatever you like. Just provide your code and a justification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:-\n",
    "#### Part 1:-\n",
    "We try to use our generated doc2vec model(as it performed better), and send it through a logistic regression model. We see that the accuracy increases by a very slight amount, from 93.7% to 93.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using Logistic Regression: 0.938507\n",
      "Recall: 0.938573 Precision: 0.938099\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn import linear_model\n",
    "clf_lr = linear_model.LogisticRegressionCV()\n",
    "clf_lr.fit(vecrep_train_doc2vec, Y_doc2vec)\n",
    "pred_lr=clf_lr.predict(vecrep_test_doc2vec)\n",
    "Y_test_lr=[labels_test[elem] for elem in range(len(labels_test))]\n",
    "print(\"Accuracy using Logistic Regression: %f\"%accuracy(pred_lr,Y_test_lr))\n",
    "evaluation(pred_lr,Y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2:-\n",
    "We then try to experiment with 2 more classifiers: Naive-Bayes and Perceptron and they both give a reduced but similar accuracies. However, the recall value increases in Naive Bayes and decreases for perceptron and the opposite is observed for precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Naive Bayes: 0.917282\n",
      "Recall: 0.940643 Precision: 0.890328\n",
      "Accuracy for Perceptron: 0.913423\n",
      "Recall: 0.899366 Precision: 0.930530\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(vecrep_train_doc2vec, Y_doc2vec)\n",
    "pred_nb=gnb.predict(vecrep_test_doc2vec)\n",
    "print(\"Accuracy for Naive Bayes: %f\"%accuracy(pred_nb,Y_test_lr))\n",
    "evaluation(pred_nb,Y_test_lr)\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf_p = Perceptron(alpha=0.001,tol=1, random_state=0)\n",
    "clf_p.fit(vecrep_train_doc2vec, Y_doc2vec)\n",
    "pred_p=clf_p.predict(vecrep_test_doc2vec)\n",
    "print(\"Accuracy for Perceptron: %f\"%accuracy(pred_p,Y_test_lr))\n",
    "evaluation(pred_p,Y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:-\n",
    "So we can conclude that the SVM classifier works as well as any other classifier in the domain. So we try to improve the doc2vec vector itself by traning the model for 100 epochs, in reference to a code at https://medium.com/@mishra.thedeepak/doc2vec-simple-implementation-example-df2afbbfbad5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atrey\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:580: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training process...\n",
      "iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atrey\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Trained\n"
     ]
    }
   ],
   "source": [
    "model_doc2vec_mod = Doc2Vec(tagged_data,size=20,window=5,\n",
    "                alpha=0.025, \n",
    "                min_alpha=0.0025,\n",
    "                min_count=1,\n",
    "                dm =1)\n",
    "\n",
    "print ('Start training process...')\n",
    "#model_doc2vec_mod.train(tagged_data, total_examples=model_doc2vec_mod.corpus_count, epochs=model_doc2vec_mod.iter)\n",
    "for epoch in range(max_epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model_doc2vec_mod.train(tagged_data,\n",
    "                total_examples=model_doc2vec_mod.corpus_count,\n",
    "                epochs=model_doc2vec_mod.iter)\n",
    "    # decrease the learning rate\n",
    "    model_doc2vec_mod.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model_doc2vec_mod.min_alpha = model_doc2vec_mod.alpha\n",
    "print ('Model Trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:-\n",
    "The computation time is drastically increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Model Trained\n",
      "Accuracy: 0.938423\n",
      "Recall: 0.937678 Precision: 0.938940\n"
     ]
    }
   ],
   "source": [
    "Y_doc2vec_mod=[labels_train[elem] for elem in range(len(labels_train))]\n",
    "vecrep_train_doc2vec_mod=[model_doc2vec_mod.docvecs[ids_train[elem]] for elem in range(len(reviews_train))]\n",
    "clf_doc2vec = svm.SVC(C=5,kernel='linear')\n",
    "clf_doc2vec.fit(vecrep_train_doc2vec_mod, Y_doc2vec_mod)  \n",
    "print(\"SVM Model Trained\")\n",
    "vecrep_test_doc2vec_mod=[model_doc2vec_mod.docvecs[ids_test[elem]] for elem in range(len(reviews_test))]\n",
    "pred_doc2vec_mod=clf_doc2vec.predict(vecrep_test_doc2vec_mod)\n",
    "print(\"Accuracy: %f\"%accuracy(pred_doc2vec_mod,Y_test))\n",
    "evaluation(pred_doc2vec_mod,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.939597\n",
      "Recall: 0.941674 Precision: 0.936922\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn import linear_model\n",
    "clf_lr = linear_model.LogisticRegressionCV()\n",
    "clf_lr.fit(vecrep_train_doc2vec_mod, Y_doc2vec_mod) \n",
    "pred_lr=clf_lr.predict(vecrep_test_doc2vec_mod)\n",
    "Y_test_lr=[labels_test[elem] for elem in range(len(labels_test))]\n",
    "print(\"Accuracy: %f\"%accuracy(pred_lr,Y_test_lr))\n",
    "evaluation(pred_lr,Y_test_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Conclusion:-\n",
    "The vector representation obtained is passed through an SVM and a logistic regression model that both outperform our original accuracy of 93.7% and it increases to 94%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: NDCG\n",
    "\n",
    "You calculated the recall and precision in Part 1 and now you get a chance to implement NDCG. \n",
    "\n",
    "Assume that Amy searches for \"food-relevant\" reviews in the **testing set** on two search engines `A` and `B`. Since the ground-truth labels for the reviews are unknown to A and B, they need to make a prediction for each review and then return a ranked list of results based on their probabilities. The results from A are in *search_result_A.json*, and the results from B are in *search_result_B.json*. Each line contains the id of a review and its corresponding ranking.\n",
    "\n",
    "You can check their labels in *review_test.json* while calculating the NDCG scores. If a review is \"food-relevant\", the relevance score is 1. Otherwise, the relevance score is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for A:-\n",
      "DCG:505.397928 IDCG:550.371256 NDCG:0.918285\n"
     ]
    }
   ],
   "source": [
    "data_resA = pd.read_json('search_result_A.json', lines=True)#Importing data for Search Result A\n",
    "[ids_resA,rank_resA]=data_resA['id'],data_resA['rank']\n",
    "labels_resA=[labels_test[list(ids_test).index(elem)] for elem in ids_resA]#Labels for search result B\n",
    "data_resB = pd.read_json('search_result_B.json', lines=True)#Importing data for Search Result B\n",
    "[ids_resB,rank_resB]=data_resB['id'],data_resB['rank']\n",
    "labels_resB=[labels_test[list(ids_test).index(elem)] for elem in ids_resB]#Labels for search result B\n",
    "import math\n",
    "def dcg(items):\n",
    "    dcg = 0\n",
    "    i = 0\n",
    "    for item in items:\n",
    "        i += 1\n",
    "        dcg += item / math.log(i + 1, 2)\n",
    "    return dcg\n",
    "# NDCG for search_result_A.json\n",
    "# Your code here\n",
    "totalrank_A=len(ids_resA)#length of the A review list\n",
    "dcg_A=dcg(labels_resA)#Calculate dcg for A\n",
    "labels_list=sorted(labels_test,reverse=True)#sort the whole corpus of the test dataset\n",
    "idcg_A=dcg(labels_list[:totalrank_A])#Take the first totalrank_A relevant labels\n",
    "print(\"Results for A:-\\nDCG:%f IDCG:%f NDCG:%f\"%(dcg_A,idcg_A,dcg_A/idcg_A))#Print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for B:-\n",
      "DCG:121.345774 IDCG:123.091533 NDCG:0.985817\n"
     ]
    }
   ],
   "source": [
    "# NDCG for search_result_B.json\n",
    "# Your code here\n",
    "totalrank_B=len(ids_resB)#length of the B review list\n",
    "dcg_B=dcg(labels_resB)#Calculate dcg for B\n",
    "idcg_B=dcg(labels_list[:totalrank_B])#Take the first totalrank_B relevant labels\n",
    "print(\"Results for B:-\\nDCG:%f IDCG:%f NDCG:%f\"%(dcg_B,idcg_B,dcg_B/idcg_B))#Print the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
